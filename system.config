### use # to comment out the configure item

################ Status ################
mode=train
# string: train/evaluate_test

################ Datasets(Input/Output) ################
datasets_fold=data/ChnSentiCorp_htl_all
train_file=ChnSentiCorp_htl_all.csv
dev_file=ChnSentiCorp_htl_all.csv

delimiter=b
# string: (t: "\t";"table")|(b: "backspace";" ")|(other, e.g., '|||', ...)

vocabs_dir=data/example_datasets_msra/vocabs

log_dir=data/example_datasets_msra/logs

checkpoints_dir=checkpoints/test

################ Labeling Scheme ################
label_scheme=BIO
# string: BIO/BIESO

label_level=2
# int, 1:BIO/BIESO; 2:BIO/BIESO + suffix
# max to 2

hyphen=-
# string: -|_, for connecting the prefix and suffix: `B_PERSON', `I_LOC'

suffix=[ORG,PERSON,LOC]
# unnecessary if label_level=1

measuring_metrics=[Accuracy]
# string: Accuracy|precision|recall|f1
# f1 is compulsory

tokenizer=CKIP
#CKIP or bert tokenizer

################ Model Configuration ################
word_input=Truec
embedding_dim=768
# int, must be consistent with `token_emb_dir' file

hidden_dim=200

max_sequence_length=256
# int, cautions! set as a LARGE number as possible,
# this will be kept during training and inferring, text having length larger than this will be truncated.

CUDA_VISIBLE_DEVICES=4
# coincides with tf.CUDA_VISIBLE_DEVICES

seed=42

################ Training Settings ###
epoch=30
#batch_size=128
batch_size=8

dropout=0.1
learning_rate=0.00002

optimizer=AdamW
# string: AdamW/SGD/Adagrad/AdaDelta/RMSprop/Adam

checkpoints_max_to_keep=30
print_per_batch=20

is_early_stop=False
patient=10
# unnecessary if is_early_stop=False

checkpoint_name=model